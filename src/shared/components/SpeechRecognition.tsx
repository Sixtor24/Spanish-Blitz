/**
 * Speech Recognition Component
 * Uses Deepgram backend streaming for all platforms (web and mobile)
 * Provides real-time transcription with interim results
 */
import { useEffect, useState, useRef, useImperativeHandle, forwardRef } from 'react';
import { Mic, MicOff } from 'lucide-react';
import { createWebSocket } from '@/config/api';

export interface SpeechRecognitionHandle {
  stop: () => void;
  isListening: () => boolean;
}

type SpeechRecognitionProps = {
  onTranscript: (transcript: string) => void;
  locale?: string;
  onError?: (error: string) => void;
  autoStop?: boolean; // If true, stops after receiving a final transcript
  stopOnCorrect?: boolean; // If true, parent can call stop() when answer is correct
};

// Constants
const MAX_DURATION = 15000; // 15 seconds maximum (safety timeout only)
const AUDIO_CHUNK_INTERVAL = 50; // 50ms for aggressive capture - faster on mobile

/**
 * Get optimal audio format - no complex detection, just use what works fastest
 */
const getAudioMimeType = (): string | undefined => {
  if (typeof window === 'undefined') return undefined;
  // Use browser default - fastest option, no detection overhead
  return undefined;
};

const SpeechRecognition = forwardRef<SpeechRecognitionHandle, SpeechRecognitionProps>(
  ({ onTranscript, locale = 'es-ES', onError, autoStop = true, stopOnCorrect = false }, ref) => {
  // State
  const [isListening, setIsListening] = useState(false);
  const [errorMessage, setErrorMessage] = useState<string | null>(null);
  const [currentTranscript, setCurrentTranscript] = useState<string>('');
  const [finalTranscript, setFinalTranscript] = useState<string>(''); // Final transcript to show below button
  const [isConnecting, setIsConnecting] = useState(false); // Track if we're in connecting/warmup phase
  
  // Refs
  const isListeningRef = useRef(false);
  const startTimeRef = useRef<number | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const wsRef = useRef<WebSocket | null>(null);
  const sessionIdRef = useRef<string | null>(null);
  const maxDurationTimeoutRef = useRef<ReturnType<typeof setTimeout> | null>(null);
  const autoStopRef = useRef(autoStop);
  const audioBufferRef = useRef<string[]>([]); // Buffer for audio chunks before WebSocket is ready
  const isWebSocketReadyRef = useRef(false); // Track if WebSocket is ready to receive audio

  /**
   * Expose methods to parent component
   */
  useImperativeHandle(ref, () => ({
    stop: () => {
      if (isListeningRef.current) {
        console.log('ðŸ›‘ [Speech] Stopping via external call');
        stopListening();
      }
    },
    isListening: () => isListeningRef.current,
  }));

  /**
   * Cleanup function
   */
  const cleanup = () => {
    console.log('ðŸ§¹ [Speech] Cleaning up resources');
    
    // Stop MediaRecorder
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      try {
        mediaRecorderRef.current.stop();
      } catch (e) {
        console.warn('MediaRecorder stop error:', e);
      }
    }
    mediaRecorderRef.current = null;
    
    // Stop media stream
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    
    // Close WebSocket
    if (wsRef.current && sessionIdRef.current) {
      try {
        wsRef.current.send(JSON.stringify({
          type: 'speech:stop',
          sessionId: sessionIdRef.current,
        }));
        wsRef.current.close();
      } catch (e) {
        console.warn('WebSocket close error:', e);
      }
    }
    wsRef.current = null;
    sessionIdRef.current = null;
    
    // Clear audio buffer
    audioBufferRef.current = [];
    isWebSocketReadyRef.current = false;
    
    // Clear max duration timeout
    if (maxDurationTimeoutRef.current) {
      clearTimeout(maxDurationTimeoutRef.current);
      maxDurationTimeoutRef.current = null;
    }
    
    // Reset state
    isListeningRef.current = false;
    startTimeRef.current = null;
  };

  /**
   * Start listening
   */
  const startListening = async () => {
    try {
      console.log('ðŸŽ¤ [Speech] Starting speech recognition');
      
      // Check if already listening
      if (isListeningRef.current) {
        console.warn('âš ï¸ [Speech] Already listening');
        return;
      }
      
      // Request microphone permission
      let stream: MediaStream;
      try {
        stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true,
          } 
        });
        streamRef.current = stream;
      } catch (err: any) {
        console.error('âŒ [Speech] Microphone permission denied:', err);
        setErrorMessage('Microphone permission required. Please allow access.');
        if (onError) onError('microphone-denied');
        return;
      }
      
      // Update state
      isListeningRef.current = true;
      setIsListening(true);
      setIsConnecting(false); // Button is interactive immediately - no connection delay
      startTimeRef.current = Date.now();
      setCurrentTranscript('Speak Now'); // Show "Speak Now" immediately
      setFinalTranscript(''); // Clear previous final transcript
      setErrorMessage(null);
      audioBufferRef.current = []; // Clear buffer
      isWebSocketReadyRef.current = false; // WebSocket not ready yet
      
      // START MEDIARECORDER IMMEDIATELY - this is the key to eliminating latency
      // User can start speaking right away, audio will be buffered
      startMediaRecorder(stream);
      
      // Create WebSocket connection IN PARALLEL while capturing audio
      const ws = createWebSocket();
      const sessionId = `speech-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;
      wsRef.current = ws;
      sessionIdRef.current = sessionId;
      
      // Setup WebSocket handlers
      ws.onopen = () => {
        console.log('ðŸ”Œ [Speech] WebSocket connected');
        ws.send(JSON.stringify({
          type: 'speech:start',
          sessionId,
          locale,
        }));
      };
      
      ws.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data);
          
          if (data.type === 'stream:started') {
            console.log('âœ… [Speech] Backend ready, flushing buffered audio...');
            isWebSocketReadyRef.current = true;
            
            // Send all buffered audio chunks to backend
            const bufferedChunks = audioBufferRef.current;
            if (bufferedChunks.length > 0) {
              console.log(`ðŸ“¤ [Speech] Sending ${bufferedChunks.length} buffered chunks`);
              bufferedChunks.forEach((base64Audio) => {
                if (ws.readyState === WebSocket.OPEN && sessionIdRef.current) {
                  ws.send(JSON.stringify({
                    type: 'speech:audio',
                    sessionId: sessionIdRef.current,
                    audio: base64Audio,
                  }));
                }
              });
              // Clear buffer after sending
              audioBufferRef.current = [];
              console.log('âœ… [Speech] Buffer flushed, now sending real-time');
            }
          }
          
          else if (data.type === 'transcript') {
            const { transcript, isFinal } = data;
            if (transcript && transcript.trim()) {
              if (!isFinal) {
                // Show interim results - clear status message when we have actual words
                setCurrentTranscript(transcript);
              } else {
                // Final result
                console.log(`ðŸ“ [Speech] Final transcript: "${transcript}"`);
                setFinalTranscript(transcript); // Store final transcript to show below
                setCurrentTranscript(''); // Clear status message
                onTranscript(transcript);
                
                // Always stop after final transcript (both correct and incorrect)
                console.log('ðŸ›‘ [Speech] Stopping after final transcript');
                setTimeout(() => {
                  stopListening();
                }, 100); // Small delay to ensure transcript is processed
              }
            }
          }
          
          else if (data.type === 'error') {
            console.error('âŒ [Speech] Backend error:', data.message);
            setErrorMessage(data.message || 'Transcription error');
            if (onError) onError('transcription-failed');
            stopListening();
          }
        } catch (error: any) {
          console.error('âŒ [Speech] Error parsing message:', error);
        }
      };
      
      ws.onerror = (error) => {
        console.error('âŒ [Speech] WebSocket error:', error);
        setErrorMessage('Connection error. Please try again.');
        if (onError) onError('connection-failed');
        stopListening();
      };
      
      ws.onclose = () => {
        console.log('ðŸ”Œ [Speech] WebSocket closed');
      };
      
    } catch (error: any) {
      console.error('âŒ [Speech] Start error:', error);
      setErrorMessage('Failed to start recording. Please try again.');
      if (onError) onError('start-failed');
      cleanup();
      setIsListening(false);
    }
  };

  /**
   * Start MediaRecorder immediately and buffer audio until WebSocket is ready
   */
  const startMediaRecorder = (stream: MediaStream) => {
    try {
      // Use browser default MIME type - fastest, no detection
      const mimeType = getAudioMimeType();
      
      // Create MediaRecorder - let browser use optimal format
      const mediaRecorder = new MediaRecorder(stream, mimeType ? { mimeType } : undefined);
      mediaRecorderRef.current = mediaRecorder;
      
      // Handle audio data - buffer or send depending on WebSocket status
      let chunkCount = 0;
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          chunkCount++;
          // Convert to base64
          const reader = new FileReader();
          reader.onloadend = () => {
            const base64Audio = (reader.result as string).split(',')[1];
            if (base64Audio && isListeningRef.current) {
              // If WebSocket is ready, send immediately
              if (isWebSocketReadyRef.current && wsRef.current?.readyState === WebSocket.OPEN && sessionIdRef.current) {
                wsRef.current.send(JSON.stringify({
                  type: 'speech:audio',
                  sessionId: sessionIdRef.current,
                  audio: base64Audio,
                }));
              } else {
                // Otherwise, buffer for later
                console.log(`ðŸ“¦ [Speech] Buffering chunk ${audioBufferRef.current.length + 1}`);
                audioBufferRef.current.push(base64Audio);
              }
            }
          };
          reader.onerror = (error) => {
            console.error(`âŒ [Speech] FileReader error:`, error);
          };
          reader.readAsDataURL(event.data);
        }
      };
      
      // Start recording immediately - user can speak right away!
      mediaRecorder.start(AUDIO_CHUNK_INTERVAL);
      console.log(`ðŸ”´ [Speech] Recording started IMMEDIATELY, capturing every ${AUDIO_CHUNK_INTERVAL}ms`);
      console.log('âœ… [Speech] User can speak NOW - audio buffered until backend ready');
      
      // Setup maximum duration timeout (safety only)
      setupSilenceDetection(stream);
      
    } catch (error: any) {
      console.error('âŒ [Speech] MediaRecorder error:', error);
      setErrorMessage('Recording failed. Please try again.');
      stopListening();
    }
  };

  /**
   * Setup maximum duration timeout (safety only)
   */
  const setupSilenceDetection = (stream: MediaStream) => {
    const maxDurationTimeout = setTimeout(() => {
      if (isListeningRef.current) {
        console.log('â±ï¸ [Speech] Max duration reached');
        stopListening();
      }
    }, MAX_DURATION);
    
    if (maxDurationTimeoutRef.current) {
      clearTimeout(maxDurationTimeoutRef.current);
    }
    maxDurationTimeoutRef.current = maxDurationTimeout;
  };


  /**
   * Stop listening
   */
  const stopListening = () => {
    console.log('ðŸ›‘ [Speech] Stopping speech recognition');
    cleanup();
    setIsListening(false);
    setIsConnecting(false);
    setCurrentTranscript('');
    // Keep finalTranscript visible until next recording starts
  };

  /**
   * Toggle listening
   */
  const toggleListening = () => {
    if (isListeningRef.current) {
      stopListening();
    } else {
      startListening();
    }
  };

  /**
   * Cleanup on unmount
   */
  useEffect(() => {
    autoStopRef.current = autoStop;
  }, [autoStop]);

  useEffect(() => {
    return () => {
      cleanup();
    };
  }, []);

  // Determine button color and text based on state
  const getButtonState = () => {
    if (errorMessage) {
      return {
        color: 'bg-red-500 hover:bg-red-600',
        text: 'Error - Try Again',
        icon: <MicOff className="w-5 h-5" />
      };
    }
    
    if (isListening) {
      // Always show "Speak Now" in orange while listening (no red "Stop Recording")
      return {
        color: 'bg-orange-500 hover:bg-orange-600',
        text: 'Speak Now',
        icon: <Mic className="w-5 h-5" />
      };
    }
    
    // Not listening - default state
    return {
      color: 'bg-green-500 hover:bg-green-600',
      text: 'Speak Answer',
      icon: <Mic className="w-5 h-5" />
    };
  };

  const buttonState = getButtonState();

  return (
    <div className="flex flex-col items-center gap-4">
      {/* Microphone Button with Dynamic State */}
      <button
        onClick={toggleListening}
        className={`
          flex items-center gap-2 px-6 py-3 rounded-lg font-medium
          transition-all duration-300 transform hover:scale-105
          ${buttonState.color} text-white shadow-lg
          ${isConnecting ? 'opacity-75 cursor-wait' : ''}
        `}
        aria-label={isListening ? 'Stop Recording' : 'Start Recording'}
        disabled={isConnecting || (!!errorMessage && !isListening)}
      >
        {buttonState.icon}
        {buttonState.text}
      </button>

      {/* Final Transcript - Show the final answer below the button */}
      {finalTranscript && !isListening && (
        <div className="px-4 py-2 bg-blue-50 text-blue-700 rounded-lg text-sm max-w-md text-center font-medium">
          {finalTranscript}
        </div>
      )}

      {/* Current Transcript (Interim Results) - Only show when actively listening and transcribing */}
      {currentTranscript && 
       isListening &&
       currentTranscript !== 'Speak Now' && (
        <div className="px-4 py-2 bg-blue-50 text-blue-700 rounded-lg text-sm max-w-md text-center">
          {currentTranscript}
        </div>
      )}

      {/* Error Message */}
      {errorMessage && (
        <div className="px-4 py-2 bg-red-50 text-red-600 rounded-lg text-sm max-w-md text-center">
          {errorMessage}
        </div>
      )}
    </div>
  );
});

SpeechRecognition.displayName = 'SpeechRecognition';

export default SpeechRecognition;
